{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"cX8P5JAm-ugw","colab_type":"code","colab":{}},"source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g9ZIYNIoa4_D","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_qV0W7Wh-7f-","colab_type":"code","colab":{}},"source":["#! -*- coding: utf-8 -*-\n","#%%\n","from __future__ import print_function\n","from keras import backend as K\n","from keras.engine.topology import Layer\n","import tensorflow as tf\n","class Position_Embedding(Layer):\n"," \n","    def __init__(self, size=None, mode='sum', **kwargs):\n","        self.size = size #必须为偶数\n","        self.mode = mode\n","        super(Position_Embedding, self).__init__(**kwargs)\n"," \n","    def call(self, x):\n","        if (self.size == None) or (self.mode == 'sum'):\n","            self.size = int(x.shape[-1])\n","        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n","        position_j = 1. / K.pow(10000., \\\n","                                 2 * K.arange(self.size / 2, dtype='float32' \\\n","                               ) / self.size)\n","        position_j = K.expand_dims(position_j, 0)\n","        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n","        position_i = K.expand_dims(position_i, 2)\n","        position_ij = K.dot(position_i, position_j)\n","        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n","        if self.mode == 'sum':\n","            return position_ij + x\n","        elif self.mode == 'concat':\n","            return K.concatenate([position_ij, x], 2)\n"," \n","    def compute_output_shape(self, input_shape):\n","        if self.mode == 'sum':\n","            return input_shape\n","        elif self.mode == 'concat':\n","            return (input_shape[0], input_shape[1], input_shape[2]+self.size)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eH2UHnnO_Rbb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRI2-NYV_S-q","colab_type":"code","colab":{}},"source":[" \n","class Attention(Layer):\n"," \n","    def __init__(self, nb_head, size_per_head, **kwargs):\n","        self.nb_head = nb_head\n","        self.size_per_head = size_per_head\n","        self.output_dim = nb_head*size_per_head\n","        super(Attention, self).__init__(**kwargs)\n"," \n","    def build(self, input_shape):\n","        self.WQ = self.add_weight(name='WQ',\n","                                  shape=(input_shape[0][-1], self.output_dim),\n","                                  initializer='glorot_uniform',\n","                                  trainable=True)\n","        self.WK = self.add_weight(name='WK',\n","                                  shape=(input_shape[1][-1], self.output_dim),\n","                                  initializer='glorot_uniform',\n","                                  trainable=True)\n","        self.WV = self.add_weight(name='WV',\n","                                  shape=(input_shape[2][-1], self.output_dim),\n","                                  initializer='glorot_uniform',\n","                                  trainable=True)\n","        super(Attention, self).build(input_shape)\n"," \n","    def Mask(self, inputs, seq_len, mode='mul'):\n","        if seq_len == None:\n","            return inputs\n","        else:\n","            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n","            mask = 1 - K.cumsum(mask, 1)\n","            for _ in range(len(inputs.shape)-2):\n","                mask = K.expand_dims(mask, 2)\n","            if mode == 'mul':\n","                return inputs * mask\n","            if mode == 'add':\n","                return inputs - (1 - mask) * 1e12\n"," \n","    def call(self, x):\n","        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n","        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n","        if len(x) == 3:\n","            Q_seq,K_seq,V_seq = x\n","            Q_len,V_len = None,None\n","        elif len(x) == 5:\n","            Q_seq,K_seq,V_seq,Q_len,V_len = x\n","        #对Q、K、V做线性变换\n","        Q_seq = K.dot(Q_seq, self.WQ)\n","        print(Q_seq)\n","        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n","        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n","        print(Q_seq)\n","        K_seq = K.dot(K_seq, self.WK)\n","        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n","        print(K_seq)\n","        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n","        V_seq = K.dot(V_seq, self.WV)\n","        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n","        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n","        print(V_seq)\n","        #计算内积，然后mask，然后softmax\n","        print(Q_seq,K_seq)\n","        A = batch_dot(Q_seq, K_seq, axes=[3,3]) / (self.size_per_head**0.5)\n","        print(A)\n","        A = K.permute_dimensions(A, (0,3,2,1))\n","        A = self.Mask(A, V_len, 'add')\n","        A = K.permute_dimensions(A, (0,3,2,1))\n","        A = K.softmax(A)\n","        #输出并mask\n","        O_seq = batch_dot(A, V_seq, axes=[3,2])\n","        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n","        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n","        O_seq = self.Mask(O_seq, Q_len, 'mul')\n","        return O_seq\n"," \n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1jMFqj7g_VDK","colab_type":"code","colab":{}},"source":["import keras\n","from keras.models import Model\n","from keras.layers import *\n","from keras.losses import sparse_categorical_crossentropy\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from keras_contrib.layers import CRF \n","from keras_contrib.losses import crf_loss\n","from keras_contrib.metrics import crf_viterbi_accuracy\n","import math"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymQHN-fd_X4q","colab_type":"code","colab":{}},"source":["X=np.load('drive//new_relation//entity//X.npy')\n","y=np.load('drive//new_relation//entity//y.npy')\n","length=np.load('drive//new_relation//entity//length.npy')\n","Tags=np.load('drive//new_relation//entity//Tags.npy')\n","vocab=np.load('drive//new_relation//entity//vocab.npy')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZ-2_O2m_eLa","colab_type":"code","colab":{}},"source":["max_seq_length=X.shape[1]\n","n_classes=len(Tags)\n","batch_size=256\n","lstm_hidden_size=256\n","validate_split=0.2\n","train_X,valid_X,train_y,valid_y=train_test_split(X,y,test_size=validate_split,random_state=520)\n","train_length,valid_length=train_test_split(length,test_size=validate_split,random_state=520)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gOCuscnX_tVa","colab_type":"code","outputId":"e08d85f7-0476-4193-b7b6-977598ca69b6","executionInfo":{"status":"ok","timestamp":1575190375609,"user_tz":-480,"elapsed":1633,"user":{"displayName":"helong xia","photoUrl":"","userId":"12754003518422150726"}},"colab":{"base_uri":"https://localhost:8080/","height":575}},"source":["from keras.models import Model\n","from keras.optimizers import SGD,Adam\n","from keras.layers import *\n","max_features = 20000\n","S_inputs = Input(shape=(max_seq_length,), dtype='int32')\n"," \n","embeddings = Embedding(max_features, 256)(S_inputs)\n","embeddings = Position_Embedding()(embeddings) #增加Position_Embedding能轻微提高准确率\n"," \n","O_seq = Attention(8,256)([embeddings,embeddings,embeddings])\n"," \n","outputs = Dense(len(Tags), activation='relu')(O_seq)\n","crf = CRF(len(Tags), sparse_target=True)(outputs)\n","\n","model = Model(inputs=S_inputs, outputs=crf)\n","# try using different optimizers and different optimizer configs\n","opt = Adam(lr=0.001)\n","#loss = 'sparse_categorical_crossentropy'\n","# model.compile(loss=loss,\n","#              optimizer=opt,\n","#              metrics=['accuracy'])\n","model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n","print(model.summary())"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Tensor(\"attention_5/Reshape_2:0\", shape=(?, 150, 2048), dtype=float32)\n","Tensor(\"attention_5/transpose_1:0\", shape=(?, 8, ?, 256), dtype=float32)\n","Tensor(\"attention_5/Reshape_7:0\", shape=(?, ?, 8, 256), dtype=float32)\n","Tensor(\"attention_5/transpose_5:0\", shape=(?, 8, ?, 256), dtype=float32)\n","Tensor(\"attention_5/transpose_1:0\", shape=(?, 8, ?, 256), dtype=float32) Tensor(\"attention_5/transpose_3:0\", shape=(?, 8, ?, 256), dtype=float32)\n","Tensor(\"attention_5/truediv:0\", shape=(?, 8, ?, ?), dtype=float32)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_5 (InputLayer)            (None, 150)          0                                            \n","__________________________________________________________________________________________________\n","embedding_5 (Embedding)         (None, 150, 256)     5120000     input_5[0][0]                    \n","__________________________________________________________________________________________________\n","position__embedding_5 (Position (None, 150, 256)     0           embedding_5[0][0]                \n","__________________________________________________________________________________________________\n","attention_5 (Attention)         (None, 150, 2048)    1572864     position__embedding_5[0][0]      \n","                                                                 position__embedding_5[0][0]      \n","                                                                 position__embedding_5[0][0]      \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 150, 29)      59421       attention_5[0][0]                \n","__________________________________________________________________________________________________\n","crf_1 (CRF)                     (None, 150, 29)      1769        dense_4[0][0]                    \n","==================================================================================================\n","Total params: 6,754,054\n","Trainable params: 6,754,054\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N0PBoUjQ_uPb","colab_type":"code","outputId":"b0b4d32f-c8f8-4ff5-8828-e6c3ce7507a8","executionInfo":{"status":"error","timestamp":1575183503934,"user_tz":-480,"elapsed":66329,"user":{"displayName":"helong xia","photoUrl":"","userId":"12754003518422150726"}},"colab":{"base_uri":"https://localhost:8080/","height":197}},"source":["from sklearn.metrics import f1_score\n","for epoch in range(100):\n","    model.fit(train_X,np.expand_dims(train_y,axis=-1),batch_size=batch_size)\n","    train_pred=model.predict(train_X[:2000])\n","    train_pred=np.reshape(np.argmax(train_pred,axis=-1),(-1))\n","    train_true=np.reshape(train_y[:2000],(-1))\n","    \n","    valid_pred=model.predict(valid_X[:2000])\n","    valid_pred=np.reshape(np.argmax(valid_pred,axis=-1),(-1))\n","    valid_true=np.reshape(valid_y[:2000],(-1))\n","    train_f1=f1_score(train_true,train_pred,average='macro')\n","    valid_f1=f1_score(valid_true,valid_pred,average='macro')\n","\n","    \n","    \n","    print('epoch {0}  f1_score {1}  valid_f1 {2} '.format(epoch,f1_score(train_true,train_pred,average='macro'),f1_score(valid_true,valid_pred,average='macro')))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/1\n","272503/272503 [==============================] - 770s 3ms/step - loss: 0.1079 - crf_viterbi_accuracy: 0.9696\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n","  'precision', 'predicted', average, warn_for)\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 0  f1_score 0.6033135788516588  valid_f1 0.5307278413808355 \n","Epoch 1/1\n","272503/272503 [==============================] - 769s 3ms/step - loss: 0.0418 - crf_viterbi_accuracy: 0.9766\n","epoch 1  f1_score 0.6844572423513838  valid_f1 0.6027589572149522 \n","Epoch 1/1\n","121856/272503 [============>.................] - ETA: 7:04 - loss: 0.0304 - crf_viterbi_accuracy: 0.9777"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yMy8MF3J_xEC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Z-Wy7EG_9dL","colab_type":"code","colab":{}},"source":["def ndim(x):\n","    \"\"\"Returns the number of axes in a tensor, as an integer.\n","\n","    # Arguments\n","        x: Tensor or variable.\n","\n","    # Returns\n","        Integer (scalar), number of axes.\n","\n","    # Examples\n","    ```python\n","        >>> from keras import backend as K\n","        >>> inputs = K.placeholder(shape=(2, 4, 5))\n","        >>> val = np.array([[1, 2], [3, 4]])\n","        >>> kvar = K.variable(value=val)\n","        >>> K.ndim(inputs)\n","        3\n","        >>> K.ndim(kvar)\n","        2\n","    ```\n","    \"\"\"\n","    dims = x.get_shape()._dims\n","    if dims is not None:\n","        return len(dims)\n","    return None\n","def batch_dot(x, y, axes=None):\n","    \"\"\"Batchwise dot product.\n","\n","    `batch_dot` is used to compute dot product of `x` and `y` when\n","    `x` and `y` are data in batch, i.e. in a shape of\n","    `(batch_size, :)`.\n","    `batch_dot` results in a tensor or variable with less dimensions\n","    than the input. If the number of dimensions is reduced to 1,\n","    we use `expand_dims` to make sure that ndim is at least 2.\n","\n","    # Arguments\n","        x: Keras tensor or variable with `ndim >= 2`.\n","        y: Keras tensor or variable with `ndim >= 2`.\n","        axes: list of (or single) int with target dimensions.\n","            The lengths of `axes[0]` and `axes[1]` should be the same.\n","\n","    # Returns\n","        A tensor with shape equal to the concatenation of `x`'s shape\n","        (less the dimension that was summed over) and `y`'s shape\n","        (less the batch dimension and the dimension that was summed over).\n","        If the final rank is 1, we reshape it to `(batch_size, 1)`.\n","\n","    # Examples\n","        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n","        `batch_dot(x, y, axes=1) = [[17], [53]]` which is the main diagonal\n","        of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n","        elements.\n","\n","        Shape inference:\n","        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n","        If `axes` is (1, 2), to find the output shape of resultant tensor,\n","            loop through each dimension in `x`'s shape and `y`'s shape:\n","\n","        * `x.shape[0]` : 100 : append to output shape\n","        * `x.shape[1]` : 20 : do not append to output shape,\n","            dimension 1 of `x` has been summed over. (`dot_axes[0]` = 1)\n","        * `y.shape[0]` : 100 : do not append to output shape,\n","            always ignore first dimension of `y`\n","        * `y.shape[1]` : 30 : append to output shape\n","        * `y.shape[2]` : 20 : do not append to output shape,\n","            dimension 2 of `y` has been summed over. (`dot_axes[1]` = 2)\n","        `output_shape` = `(100, 30)`\n","\n","    ```python\n","        >>> x_batch = K.ones(shape=(32, 20, 1))\n","        >>> y_batch = K.ones(shape=(32, 30, 20))\n","        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n","        >>> K.int_shape(xy_batch_dot)\n","        (32, 1, 30)\n","    ```\n","    \"\"\"\n","    if isinstance(axes, int):\n","        axes = (axes, axes)\n","    x_ndim = ndim(x)\n","    y_ndim = ndim(y)\n","    if axes is None:\n","        # behaves like tf.batch_matmul as default\n","        axes = [x_ndim - 1, y_ndim - 2]\n","\n","    if x_ndim > y_ndim:\n","        diff = x_ndim - y_ndim\n","        y = tf.reshape(y, tf.concat([tf.shape(y), [1] * (diff)], axis=0))\n","    elif y_ndim > x_ndim:\n","        diff = y_ndim - x_ndim\n","        x = tf.reshape(x, tf.concat([tf.shape(x), [1] * (diff)], axis=0))\n","    else:\n","        diff = 0\n","    if ndim(x) == 2 and ndim(y) == 2:\n","        if axes[0] == axes[1]:\n","            out = tf.reduce_sum(tf.multiply(x, y), axes[0])\n","        else:\n","            out = tf.reduce_sum(tf.multiply(tf.transpose(x, [1, 0]), y), axes[1])\n","    else:\n","        if axes is not None:\n","            adj_x = None if axes[0] == ndim(x) - 1 else True\n","            adj_y = True if axes[1] == ndim(y) - 1 else None\n","        else:\n","            adj_x = None\n","            adj_y = None\n","        out = tf.matmul(x, y, adjoint_a=adj_x, adjoint_b=adj_y)\n","    if diff:\n","        if x_ndim > y_ndim:\n","            idx = x_ndim + y_ndim - 3\n","        else:\n","            idx = x_ndim - 1\n","        out = tf.squeeze(out, list(range(idx, idx + diff)))\n","    if ndim(out) == 1:\n","        out = expand_dims(out, 1)\n","    return out\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9bY9koBALnj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWwf6LtSMtsS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UPp0VQBUM1XC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxgzF2K6M5VS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xiq9BNn8NEXx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mV-DxV_CNfAa","colab_type":"code","colab":{}},"source":["!pip install git+https://www.github.com/keras-team/keras-contrib.git\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-iFMdW_dbfTh","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}